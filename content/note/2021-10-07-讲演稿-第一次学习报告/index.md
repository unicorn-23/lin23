---
title: 讲演稿-第一次学习报告
author: Lin Gui
date: '2021-10-07'
slug: 讲演稿-第一次学习报告
categories:
  - 学习
tags:
  - 学习报告
---

额，大家好啊，开学以来第一次以这种形式和大家交流分享，这次呢由我来给大家大致的汇报一下从开学到现在一个月以来，大概都学了点什么，我的学习笔记和学习路径有在博客中记录，欢迎大家和我交流学习。

【点鼠标】

这次汇报呢我从两个方面进行展开，一个是所学的知识点，一个是所看的文献。知识点方面，我主要学习了这三个，标准神经网络Standard Neural NetWork、卷积神经网络CNN、图神经网络GNN以及循环神经网络RNN，其中卷积和图学的相对多一点。下面简单讲一下神经网络和卷积，大家可能有还没开始学的，希望我能让你初步了解一下神经网络相关知识，也可能大家已经熟练掌握了，也希望你们能够耐心听一下，指出我的不足。好的。那我开始讲了。

【点鼠标】

看，这就是一个最简单的神经网络模型，他只有一个神经元，其中X是输入，Y是输出，而这个神经元则代表了某些运算，可能是最简单的运算,比如这个线性运算y=wx，这就是一个最简单的神经网络。

【点鼠标】

我们再看一下这个稍微复杂一点的神经网络，这里呢有很多个输入x，这一列呢叫输入层，他们传入这两层神经元，在这两层神经元中进行某些运算，然后获得输出值y，而其中这两层的具体运算过程呢是由神经网络自己决定的。那么这两层具体干了点什么呢，也就是这个神经网络具体是怎么运行的呢。

【点鼠标】

这里我们先来看一个二分类问题，了解一下什么是loss function和cost function。

【点鼠标】

假设我们给出一个训练集（X，Y），他由m个训练样本组成，其中x是n维的特征向量，因为这里是二分类问题，所以y的值是0或1。

【点鼠标】

我们将训练集中的输入X放入输入层，经过两层神经元之后，将会得到这个预测值y一撇，而这个y一撇的值是输出等于1时的概率。

【点鼠标】

我们给出两个参数，w和b，这个w是n维的，b是实数。

【点鼠标】

假设这个神经元中发生的运算是线性运算，也就是预测值y一撇等于wx加上b，但如果是这样的话，y撇的值就有可能大于1或者是负数，不符合我们期望的y撇的取值范围，因为概率值是在0到1之间的，所以要给这个式子套上一个函数，叫sigmoid函数

【点鼠标】

可以看到当套了这个sigmoid函数之后，不管大于1的数还是负的数都会变成0到1之间。

【点鼠标】

所以输出y一撇就变成了这个样子。这个sigma在这里就是sigmoid函数。好的，那现在我们已经假设了：输入样本X经过神经网络运算之后，得到了的输出也就是预测值y撇，而我们的目标是将输入样本进行准确的二分类问题，那在样本中还有这个Y，代表了真实值，那我们现在要判断一下预测值与真实值之间的差距是多少，方便我们通过对w和b的调整，来对差距进行缩小，这里我们引入Loss function和这个Cost function 的概念。

【点鼠标】

首先讲一下Loss function，这个Loss function是用来衡量单个训练样本上得到的预测值y撇和真实值y之间有多接近的函数，而这个Cost function则是代表了全体训练样本的表现，因为是全体训练样本，所以这里有个m分之1。通常衡量两者误差大小可以使用误差平方法，误差平方的值越小，说明预测值与真实值越接近。因此使预测值变准确的过程，对于单个样本就是最小化Loss function的过程，这也就是神经网络优化的目标。而最小化Loss function的方法是梯度下降，那什么是梯度下降呢

【点鼠标】

可以看到这里有个曲面，水平轴w和b是需要进行学习的两个参数，纵轴代表Loss function的值，假如这个曲面上有个点

【点鼠标】

我们要优化这个点，也就是要他朝着曲面的最低点前进

【点鼠标】

我们来看这个简化过的曲线，也就是只看w轴和纵轴的情况，我们要做的就是使这个点，沿着这条曲线向下移动，找到最小值。

【点鼠标】

这里的参数alpha是指学习率，学习率乘以J(w)的导数，就代表了他一步向下移动多少距离，而参数w不断迭代更新自身，就能逐步达到J(w)的最小值

【点鼠标】

反过来看这里的曲面也是一样的，用这两个偏导来更新w和b的值。

【点鼠标】

但是用误差平方作为Loss function的话，会有个问题，就是可能会使刚刚的优化问题变成非凸的，导致不能求出全局最优解，因此，我们通常会使用这个交叉熵来做为损失函数，使优化问题变成凸的，从而能够求得全局最优解，也就是全局最小值。

【点鼠标】

好的，讲好了loss function之后，我们来看一下，神经网络具体的计算过程，就是像这样子

【点鼠标】

首先将输入传到隐藏层，为什么叫隐藏层，因为作为用户，我们只能看到输入和输出，所以除了输入层和输出层别的都叫隐藏层，而这个隐藏层中的每个神经元

【点鼠标】

就像这样子，里面都包含一个线性函数z=wx+b，和一个激活函数sigma(z)，这个激活函数可以是sigmoid函数或者是其他的一些激活函数，为什么要使用激活函数呢？因为如果不使用激活函数，那每层都相当于矩阵相乘。如果使用的话，激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数。

【点鼠标】

【点鼠标】

然后将第一层隐藏层的计算结果传入第二层隐藏层，我们称它为输出层

【点鼠标】

为什么叫输出层呢？因为输出层对传入的值进行计算会得到y撇，而这个y撇就是我们的输出预测值。这个蓝色箭头的传播过程称之为正向传播。

而对应的反向传播的过程是这样的

【点鼠标】

通过y撇呢我们能够得到loss function的值，也就是L（y撇，y），我们单独看一下输出层

【点鼠标】

因为y撇是在输出层计算后的结果，也就是这个a的值，在这里将y撇用a代替表示

【点鼠标】

所以这里lossfunction用L(a,y)表示，将L(a,y)对a进行求导，得到da等于这个

【点鼠标】

然后根据链导法则，我们可以得出dz等于da乘上sigma(z)的导数

【点鼠标】

同理，可以得出dw和db的值

【点鼠标】

再根据我们之前说的这个梯度下降法，将w减去学习率乘上dw的值重新赋值给w，将b减去学习率乘上db的值重新赋值给b，就完成了一轮参数的更新，这个是输出层的反向传播过程。

【点鼠标】

【点鼠标】

在其他的隐藏层也是一样，用刚才的方法更新隐藏层中每个神经元里的参数w和b的值。再通过更新好的w和b，再一次进行正向传播，得到loss function的值，再从loss function进行反向传播，来不断学习里面的参数w和b，再学习过程中，loss function的值会不断减少，当loss function的值不再下降时，就停止运行神经网络，此时也就获得了w和b的最优值。那这个正向传播和反向传播的过程循环的过程就是标准神经网络的运行过程。

【点鼠标】

大概了解了标准神经网络之后呢，我们来看一下这个卷积神经网络，这个卷积神经网络呢主要是用来处理图像数据的，他和普通的神经网络区别就在于他有一个叫卷积核的东西，也就是这个filter，他的作用呢是提取这个图像的特征

【点鼠标】

那他是怎么提取的呢，我们来看一下，这里有一个图片输入，他是一个7乘7的矩阵，里面的特征是1和-1，这个卷积核呢长这样，是一个3乘3的矩阵，他的主对角线全为1，其余位置是-1。我们将这个卷积核和输入的左上角对齐，将对应位置上的数字相乘，再相加，然后放到右侧的这个输出矩阵的第一个位置上

【点鼠标】

然后我们把卷积核向右移动一格，这个移动的距离，称为步长，假设我们现在步长为一，那和刚才的操作一样，在右边的矩阵的第二个位置就得到了-1/9，大约是-0.1

【点鼠标】

以此类推

【点鼠标】

我们可以得到一个输出的矩阵

【点鼠标】

就像这样，这个矩阵叫一个feature map，这个feature map上的每一个点都是由卷积核在原图上进行卷积操作得出的，也就是这个点聚合了原图局部的特征。

【点鼠标】

现在我们换一个卷积核，他是副对角线上为1，【点鼠标】，得到的feature map是这样的

【点鼠标】

我们再使用一个两条对角线都为1的卷积核

【点鼠标】

他得出的feature map是这样的。

【点鼠标】

总的来看，我们输入一个图像样本，他经过不同的卷积核卷积之后，会得到与卷积核相同数量的feature map，我们可以给feature map套上一个激活函数，对他做一下归一化的操作。这里我们使用ReLU function 作为激活函数。

【点鼠标】

ReLU函数是这样的，也就是将小于零的数置零，其余数不变

【点鼠标】

我们将左边的矩阵用ReLU进行归一化之后就能得到右侧的矩阵

【点鼠标】

这里还有个操作叫Pooling，池化操作，目的是缩小feature map的大小，提高计算的效率。Pooling的方法呢有很多，常见的有最大和求平均这两种，他们池化的效果如图所示，这个最大池化，可以看到右侧蓝色的部分是左侧蓝色部分四个值中最大那个数，其余也是一样，平均池化则是将相同颜色相加求平均。池化操作可以将原来的矩阵缩小一半，同时保留原来的部分特征。

【点鼠标】

我们将刚才的feature map进行最大池化操作，得到右侧的三个新矩阵。可以看到这三个矩阵还是拥有左侧feature map的主要特征的。

【点鼠标】

接下来进行全链接操作，也就是将图像展开变成一维的，当中这一列叫全链接层，这也就变成了标准神经网络的输入层，随后我们就能像运行标准神经网络一样对这些数据进行处理，最后判断出他具体是什么图型。我们可以通过重复进行卷积、归一化和池化这三个操作，来更好的提取图像的特征。

【点鼠标】

当然卷积神经网络不只能够用在图像处理上，他同时在我们研究的生物信息方向也有用武之地。我最早挑的一个论文方向是蛋白质二级结构的预测，主要看的是这篇使用生成对抗网络和卷积神经网络预测蛋白质二级结构的文章，还有这篇关于蛋白质二级结构预测的深度监督卷积生成随机网络，第二篇我主要是看作者写的代码。当然我也参考了一些其他的综述和文献帮助我理解这个蛋白质二级结构预测的流程，但是因为没有做好相关的文献管理工作，现在想回顾一下，也不知道当初具体看了哪些文章，当然现在也可以从浏览器的历史记录中寻回看过的文章，但比较麻烦，希望大家在文献管理这个问题上能吸取我的教训，引以为戒吧。

额，这里就讲一下第一篇。

【点鼠标】

首先看一下他的摘要，我们可以了解到这篇文章的实验方法是利用对抗网络和卷积神经网络来预测蛋白质二级结构，实验结果看起来也是不错的。

【点鼠标】

那这篇文章相对于普通的卷积神经网络预测蛋白质二级结构的创新点呢在于，这个作者使用了生成对抗网络来提取数据的特征，再将融合好的特征放到卷积神经网络里进行预测。

【点鼠标】

这个是他的实验流程。可以看到同一份数据集，他进行了两种操作，来互相做对比，一种是普通使用卷积神经网络，一种是刚刚讲的，也就是他的创新的地方。

【点鼠标】

然后这是他使用的数据集，这个作者使用的数据集是真的难找，这里我说一下自己的感受，如果要做文章复现的话，最好是找那种能找到源码和附了数据集的文章，我当初花了大把时间在找数据集上，有点得不偿失，尤其是这个PDB25，找了好久，但很多数据集最后也没用上。然后这个训练集当中的Cull PDB倒是在别的文章中也挺常用的。

【点鼠标】

这是他使用卷积神经网络进行预测的过程

【点鼠标】

这是生成对抗网络模型，G是生成器和D是鉴别器，这个生成对抗网络的作用是创造新的假样本，但是在我们看来这个生成的假样本也是比较真的，作者的行为在我看来就是扩大原有的训练集，来让训练获得更好的效果。

【点鼠标】

文中使用滑动窗口的长度分别为13和19，卷积层的卷积核的大小和尺寸在窗口长度为13时是11\*11\*270、11\*11\*160和在19时是19\*19\*290、16\*16\*170。在处理图像的时候这个滑动窗口大小意味着将图片分成一块块的，然后把这一块块分别放去卷积操作。但现在是在处理蛋白质序列，而蛋白质序列是字符串形式的，所以这个窗口大小也就意味着卷积网络一次要处理的序列长度。

【点鼠标】

可以看到这个窗口长度是19的时候，预测准确率还是比窗口长度等于13的时候要好一点的，这个19比13好的原因呢是因为使用长度为19的窗口可以包含更多的蛋白质特征信息。

【点鼠标】

然后作者找了几个对照组也就是别人的模型，来凸显自己的模型有多强。

然后讲一下我的复现过程，首先一开始我是盯着这篇文章准备做复现的，但我文章看的不够多，不知道文章还能自带源码，他这个文章呢没有源码，也没有附数据集，我是照着他的文章一步步来，光是找数据集就花了很多时间。

【点鼠标】

但是在找的数据集过程中呢，也顺便学习了一点生信方面的知识，比如使用PSI- BLAST来生成这个蛋白质位置序列特异性矩阵PSSM，但是当时我只知道这个PSSM是作为特征输入神经网络，他是一个m*20的矩阵，其中m指序列长度，20指20个氨基酸。但是我始终不太清楚这个放入神经网络的数据形式具体是什么样的。

所以最后感觉这篇文章的复现还是比较难做的，于是就另找了一篇带源码的，也就是刚刚说的的第二篇，学习了一下他的数据处理和代码思路

【点鼠标】

首先他使用的数据集是CB513，这个是一个已经处理好的数据集，用起来就很舒服了，可以看到这个数据集能用numpy进行处理，可以通过reshape，把数据集转换成N乘700乘57的形式，然后就能用了。

【点鼠标】

大概讲一下代码实现的思路，我将代码分为4部分，也就是数据处理、构建模型、训练+测试以及画图，就简单的讲一下，因为做的比较粗糙，就不给大家细看了，其中比较复杂的部分是数据处理，要将数据reshape成模型想要的样子。

【点鼠标】

【点鼠标】

主要就是这样子的，数据处理模块里再分成划分训练集、测试集模块和改变训练集形状模块

【点鼠标】

然后这个模型的部分，因为这里我使用的是tensorflow的框架，只要一层层加进去就行了，需要注意的地方就是矩阵前后的形状要一致，不然会debug很久。然后运行的时候像反向传播之类的，这个tensorflow框架都处理好了，直接调用就行了。

画图都画用matplotlib，将训练过程中产生的数据存在日志里，然后画一下就行了。

【点鼠标】

然后刚刚讲的这些是卷积网络预测蛋白质二级结构的部分。开学这个月我还学习了一点图神经网络之类的知识，额时间关系吧，我就先讲这些，当然CNN当中也有很多细节我没有展开，如果大家有什么问题，欢迎和我探讨，也希望大家能够指出我的不足之处。那就先这样，谢谢大家。

